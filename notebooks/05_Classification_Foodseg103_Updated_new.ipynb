{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyPIFMTlZL6+Yv4i10GsreB4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# ==================================================\n","# Cell 1: Setup environment, mount Drive, define paths\n","# ==================================================\n","\n","!pip install -q timm==0.9.16\n","\n","import os, random, time, pickle, copy\n","from pathlib import Path\n","from collections import Counter, defaultdict\n","\n","import numpy as np\n","from PIL import Image, UnidentifiedImageError\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.cuda.amp import GradScaler, autocast\n","\n","import timm\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","\n","# Reproducibility\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", DEVICE)\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","\n","# NEW PROJECT ROOT\n","PROJECT_ROOT = Path(\"/content/drive/MyDrive/Cal_Estimation_Project\")\n","IMG_DIR = PROJECT_ROOT / \"data/raw/foodseg103/FoodSeg103/Images/img_dir\"\n","ANN_DIR = PROJECT_ROOT / \"data/raw/foodseg103/FoodSeg103/Images/ann_dir\"\n","CAT_FILE = PROJECT_ROOT / \"data/raw/foodseg103/FoodSeg103/category_id.txt\"\n","\n","print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n","print(\"IMG_DIR     :\", IMG_DIR, \"| exists?\", IMG_DIR.exists())\n","print(\"ANN_DIR     :\", ANN_DIR, \"| exists?\", ANN_DIR.exists())\n","print(\"CAT_FILE    :\", CAT_FILE, \"| exists?\", CAT_FILE.exists())\n","\n","assert IMG_DIR.exists() and ANN_DIR.exists() and CAT_FILE.exists(), \"Fix paths in Cell 1 before continuing.\"\n","\n","CKPT_DIR = PROJECT_ROOT / \"checkpoints\"\n","CKPT_DIR.mkdir(parents=True, exist_ok=True)\n","\n","def gpu_clear():\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"geX_GnncxKy3","executionInfo":{"status":"ok","timestamp":1764258462618,"user_tz":360,"elapsed":6642,"user":{"displayName":"Shaik Mathin","userId":"03614909128096919146"}},"outputId":"7fd8756d-0b85-4c6b-ecf7-b8710164dadb"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n","Mounted at /content/drive\n","PROJECT_ROOT: /content/drive/MyDrive/Cal_Estimation_Project\n","IMG_DIR     : /content/drive/MyDrive/Cal_Estimation_Project/data/raw/foodseg103/FoodSeg103/Images/img_dir | exists? True\n","ANN_DIR     : /content/drive/MyDrive/Cal_Estimation_Project/data/raw/foodseg103/FoodSeg103/Images/ann_dir | exists? True\n","CAT_FILE    : /content/drive/MyDrive/Cal_Estimation_Project/data/raw/foodseg103/FoodSeg103/category_id.txt | exists? True\n"]}]},{"cell_type":"code","source":["# ==================================================\n","# Cell 2: Cache utilities for fast re-runs (NEW)\n","# ==================================================\n","\n","CACHE_DIR = PROJECT_ROOT / \"cache\"\n","CACHE_DIR.mkdir(parents=True, exist_ok=True)\n","\n","def save_cache(obj, name=\"05_cache\"):\n","    path = CACHE_DIR / f\"{name}.pkl\"\n","    with open(path, \"wb\") as f:\n","        pickle.dump(obj, f)\n","    print(f\"✔ Saved cache → {path}\")\n","\n","def load_cache(name=\"05_cache\"):\n","    path = CACHE_DIR / f\"{name}.pkl\"\n","    if path.exists():\n","        print(f\"✔ Loaded cache → {path}\")\n","        with open(path, \"rb\") as f:\n","            return pickle.load(f)\n","    return None\n"],"metadata":{"id":"EFKdLG4BxM-s","executionInfo":{"status":"ok","timestamp":1764258462621,"user_tz":360,"elapsed":1,"user":{"displayName":"Shaik Mathin","userId":"03614909128096919146"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# ==================================================\n","# Cell 3: Load category_id.txt and build id->name\n","# ==================================================\n","\n","cat_id2name = {}\n","with open(CAT_FILE, \"r\") as f:\n","    for line in f:\n","        line = line.strip()\n","        if not line:\n","            continue\n","        parts = line.split()\n","        cid = int(parts[0])\n","        name = \" \".join(parts[1:])\n","        cat_id2name[cid] = name\n","\n","print(f\"Loaded {len(cat_id2name)} entries from category_id.txt\")\n","print(\"Examples:\", list(cat_id2name.items())[:10])\n","\n","all_class_ids = sorted([cid for cid in cat_id2name.keys() if cid != 0])\n","print(\"Total non-background food classes:\", len(all_class_ids))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0LqFQmfkxXuE","executionInfo":{"status":"ok","timestamp":1764258462625,"user_tz":360,"elapsed":3,"user":{"displayName":"Shaik Mathin","userId":"03614909128096919146"}},"outputId":"c9498901-2cc2-45a9-dc02-f9827bd1ceed"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 104 entries from category_id.txt\n","Examples: [(0, 'background'), (1, 'candy'), (2, 'egg tart'), (3, 'french fries'), (4, 'chocolate'), (5, 'biscuit'), (6, 'popcorn'), (7, 'pudding'), (8, 'ice cream'), (9, 'cheese butter')]\n","Total non-background food classes: 103\n"]}]},{"cell_type":"code","source":["# ==================================================\n","# Cell 4: Build and cache all usable img/mask pairs\n","# ==================================================\n","\n","pairs_cache = PROJECT_ROOT / \"foodseg103_pairs_cache.pkl\"\n","\n","if pairs_cache.exists():\n","    print(\"Loading usable_pairs + all_ids_per_pair from cache:\", pairs_cache)\n","    with open(pairs_cache, \"rb\") as f:\n","        data = pickle.load(f)\n","    usable_pairs = data[\"usable_pairs\"]\n","    all_ids_per_pair = data[\"all_ids_per_pair\"]\n","else:\n","    print(\"Scanning masks to build usable_pairs (one time)...\")\n","    split_names = [\"train\", \"val\", \"test\"]\n","    usable_pairs = []\n","    all_ids_per_pair = []\n","\n","    for split in split_names:\n","        img_dir_split = IMG_DIR / split\n","        ann_dir_split = ANN_DIR / split\n","        if not img_dir_split.exists():\n","            continue\n","        for img_name in sorted(os.listdir(img_dir_split)):\n","            if not img_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n","                continue\n","            img_path = img_dir_split / img_name\n","            ann_path = ann_dir_split / (img_name.rsplit(\".\", 1)[0] + \".png\")\n","            if not ann_path.exists():\n","                continue\n","            try:\n","                mask = np.array(Image.open(ann_path))\n","            except UnidentifiedImageError:\n","                print(\"[WARN] Skipping unreadable mask:\", ann_path)\n","                continue\n","\n","            ids = set(np.unique(mask).tolist())\n","            ids.discard(0)\n","            if len(ids) == 0:\n","                continue\n","\n","            usable_pairs.append((img_path, ann_path))\n","            all_ids_per_pair.append(ids)\n","\n","    with open(pairs_cache, \"wb\") as f:\n","        pickle.dump(\n","            {\n","                \"usable_pairs\": usable_pairs,\n","                \"all_ids_per_pair\": all_ids_per_pair,\n","            },\n","            f,\n","        )\n","    print(\"Cached to:\", pairs_cache)\n","\n","print(\"Total usable pairs:\", len(usable_pairs))\n","print(\"Example pair:\", usable_pairs[0])\n","print(\"Example ids:\", list(all_ids_per_pair[0])[:10])\n","\n","gpu_clear()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aaS6Qp7OxZW_","executionInfo":{"status":"ok","timestamp":1764258462628,"user_tz":360,"elapsed":2,"user":{"displayName":"Shaik Mathin","userId":"03614909128096919146"}},"outputId":"c144c29d-477b-4f21-8abb-5055499fbe1d"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading usable_pairs + all_ids_per_pair from cache: /content/drive/MyDrive/Cal_Estimation_Project/foodseg103_pairs_cache.pkl\n","Total usable pairs: 7118\n","Example pair: (PosixPath('/content/drive/MyDrive/Cal_Estimation_Project/data/raw/foodseg103/FoodSeg103/Images/img_dir/train/00000000.jpg'), PosixPath('/content/drive/MyDrive/Cal_Estimation_Project/data/raw/foodseg103/FoodSeg103/Images/ann_dir/train/00000000.png'))\n","Example ids: [48, 90, 66]\n"]}]},{"cell_type":"code","source":["# ==================================================\n","# Cell 5: Select images, enforce coverage, split train/val\n","# ==================================================\n","\n","N_IMAGES = 3000  # or None to use all\n","\n","rng = np.random.RandomState(SEED)\n","num_pairs = len(usable_pairs)\n","indices = list(range(num_pairs))\n","\n","covered = set()\n","selected_indices = []\n","\n","for cid in all_class_ids:\n","    candidates = [i for i, s in enumerate(all_ids_per_pair) if cid in s]\n","    if not candidates:\n","        print(f\"[WARN] No images for class id {cid} ({cat_id2name.get(cid, '?')})\")\n","        continue\n","    chosen = rng.choice(candidates)\n","    selected_indices.append(chosen)\n","    covered.add(cid)\n","\n","selected_indices = sorted(set(selected_indices))\n","print(f\"After coverage: {len(selected_indices)} selected (unique)\")\n","\n","if (N_IMAGES is None) or (N_IMAGES <= 0) or (N_IMAGES > num_pairs):\n","    target_n = num_pairs\n","else:\n","    target_n = N_IMAGES\n","\n","remaining = [i for i in indices if i not in selected_indices]\n","rng.shuffle(remaining)\n","while len(selected_indices) < target_n and remaining:\n","    selected_indices.append(remaining.pop())\n","selected_indices = sorted(selected_indices)\n","print(f\"Total selected images: {len(selected_indices)} (target {target_n})\")\n","\n","selected_pairs = [usable_pairs[i] for i in selected_indices]\n","selected_ids_per_pair = [all_ids_per_pair[i] for i in selected_indices]\n","\n","rng.shuffle(selected_indices)\n","split_point = int(0.8 * len(selected_indices))\n","train_img_indices = set(selected_indices[:split_point])\n","val_img_indices   = set(selected_indices[split_point:])\n","\n","print(f\"Train images: {len(train_img_indices)} | Val images: {len(val_img_indices)}\")\n","\n","gpu_clear()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wk3Jwb4Xxbjq","executionInfo":{"status":"ok","timestamp":1764258462732,"user_tz":360,"elapsed":2,"user":{"displayName":"Shaik Mathin","userId":"03614909128096919146"}},"outputId":"c33b1cd7-c79a-4e6c-8315-3ed753fcc184"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["After coverage: 102 selected (unique)\n","Total selected images: 3000 (target 3000)\n","Train images: 2400 | Val images: 600\n"]}]},{"cell_type":"code","source":["# ==================================================\n","# Cell 6: Build or load segment crops (masked) for training/validation\n","# ==================================================\n","\n","MIN_AREA = 300   # min number of pixels in mask\n","PAD_FRAC = 0.05  # padding fraction around bbox\n","\n","segments_cache = PROJECT_ROOT / \"foodseg103_segments_convnextL_masked.pkl\"\n","\n","if segments_cache.exists():\n","    print(f\"Loading segments from cache: {segments_cache}\")\n","    with open(segments_cache, \"rb\") as f:\n","        data = pickle.load(f)\n","    segments = data[\"segments\"]\n","    train_segments = data[\"train_segments\"]\n","    val_segments = data[\"val_segments\"]\n","    print(f\"Loaded from cache -> Total segments: {len(segments)}\")\n","    print(f\"Train segments: {len(train_segments)} | Val segments: {len(val_segments)}\")\n","\n","else:\n","    print(\"No segments cache found. Building segments from masks (one-time heavy step)...\")\n","    segments = []\n","\n","    total_sel = len(selected_indices)\n","    for k, global_idx in enumerate(selected_indices):\n","        img_path, ann_path = usable_pairs[global_idx]\n","        split = \"train\" if global_idx in train_img_indices else \"val\"\n","\n","        if k % 100 == 0:\n","            print(f\"[{k}/{total_sel}] Processing idx={global_idx} -> {img_path.name}\")\n","\n","        try:\n","            img = Image.open(img_path).convert(\"RGB\")\n","            mask = np.array(Image.open(ann_path))\n","        except UnidentifiedImageError:\n","            print(f\"[WARN] Skipping unreadable {img_path} / {ann_path}\")\n","            continue\n","\n","        h, w = mask.shape\n","        ids_here = np.unique(mask)\n","        ids_here = ids_here[ids_here != 0]\n","        if ids_here.size == 0:\n","            continue\n","\n","        for cid in ids_here:\n","            ys, xs = np.where(mask == cid)\n","            if xs.size < MIN_AREA:\n","                continue\n","\n","            x0, x1 = xs.min(), xs.max()\n","            y0, y1 = ys.min(), ys.max()\n","\n","            pad = int(PAD_FRAC * max(h, w))\n","            x0p = max(0, x0 - pad)\n","            y0p = max(0, y0 - pad)\n","            x1p = min(w - 1, x1 + pad)\n","            y1p = min(h - 1, y1 + pad)\n","\n","            segments.append(\n","                {\n","                    \"img_idx\": int(global_idx),\n","                    \"split\": split,\n","                    \"img_path\": str(img_path),\n","                    \"ann_path\": str(ann_path),\n","                    \"class_id\": int(cid),\n","                    \"class_name\": cat_id2name.get(int(cid), f\"class_{cid}\"),\n","                    \"bbox\": (int(x0p), int(y0p), int(x1p), int(y1p)),\n","                }\n","            )\n","\n","    print(f\"\\nFinished building segments. Total segments: {len(segments)}\")\n","\n","    train_segments = [s for s in segments if s[\"split\"] == \"train\"]\n","    val_segments   = [s for s in segments if s[\"split\"] == \"val\"]\n","\n","    print(f\"Train segments: {len(train_segments)} | Val segments: {len(val_segments)}\")\n","\n","    with open(segments_cache, \"wb\") as f:\n","        pickle.dump(\n","            {\n","                \"segments\": segments,\n","                \"train_segments\": train_segments,\n","                \"val_segments\": val_segments,\n","            },\n","            f,\n","        )\n","    print(\"Saved segments cache to:\", segments_cache)\n","\n","gpu_clear()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0SjMBN2WxeEo","executionInfo":{"status":"ok","timestamp":1764258462782,"user_tz":360,"elapsed":49,"user":{"displayName":"Shaik Mathin","userId":"03614909128096919146"}},"outputId":"9c559676-4121-4d34-f2fe-867bbffdf709"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading segments from cache: /content/drive/MyDrive/Cal_Estimation_Project/foodseg103_segments_convnextL_masked.pkl\n","Loaded from cache -> Total segments: 10934\n","Train segments: 8778 | Val segments: 2156\n"]}]},{"cell_type":"code","source":["# ==================================================\n","# Cell 7: Dataset, transforms, dataloaders, class mapping\n","# ==================================================\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Class mapping: use class names (human-readable)\n","all_class_names = sorted({s[\"class_name\"] for s in segments})\n","class2idx = {name: i for i, name in enumerate(all_class_names)}\n","idx2class = {i: name for name, i in class2idx.items()}\n","NUM_CLASSES = len(class2idx)\n","\n","print(\"Num classes:\", NUM_CLASSES)\n","print(\"Example class2idx entries:\", list(class2idx.items())[:10])\n","\n","IMG_SIZE = 288\n","\n","train_transform = transforms.Compose(\n","    [\n","        transforms.Resize((IMG_SIZE + 32, IMG_SIZE + 32)),\n","        transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ColorJitter(\n","            brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05\n","        ),\n","        transforms.RandomRotation(10),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","            mean=[0.485, 0.456, 0.406],\n","            std=[0.229, 0.224, 0.225],\n","        ),\n","    ]\n",")\n","\n","val_transform = transforms.Compose(\n","    [\n","        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","            mean=[0.485, 0.456, 0.406],\n","            std=[0.229, 0.224, 0.225],\n","        ),\n","    ]\n",")\n","\n","\n","class SegmentDataset(Dataset):\n","    def __init__(self, segments, class2idx, transform, use_mask=True, return_meta=False):\n","        self.segments = segments\n","        self.class2idx = class2idx\n","        self.transform = transform\n","        self.use_mask = use_mask\n","        self.return_meta = return_meta\n","\n","    def __len__(self):\n","        return len(self.segments)\n","\n","    def __getitem__(self, idx):\n","        s = self.segments[idx]\n","\n","        # --- Load full image & mask ---\n","        img = Image.open(s[\"img_path\"]).convert(\"RGB\")\n","        img_np = np.array(img)  # (H_img, W_img, 3)\n","\n","        mask_full = np.array(Image.open(s[\"ann_path\"]))\n","        if mask_full.ndim == 3:\n","            mask_full = mask_full[:, :, 0]  # (H_mask, W_mask)\n","\n","        h_img, w_img, _ = img_np.shape\n","        h_mask, w_mask = mask_full.shape\n","\n","        x0, y0, x1, y1 = s[\"bbox\"]\n","\n","        # --- Clamp bbox to both image and mask bounds ---\n","        x0 = max(0, min(x0, w_img - 1, w_mask - 1))\n","        y0 = max(0, min(y0, h_img - 1, h_mask - 1))\n","        x1 = max(x0, min(x1, w_img - 1, w_mask - 1))\n","        y1 = max(y0, min(y1, h_img - 1, h_mask - 1))\n","\n","        # --- Crop image and mask with same bbox ---\n","        crop_np = img_np[y0 : y1 + 1, x0 : x1 + 1, :]            # (Hc_img, Wc_img, 3)\n","        cid = s[\"class_id\"]\n","        mask_local = (mask_full[y0 : y1 + 1, x0 : x1 + 1] == cid)  # (Hc_mask, Wc_mask)\n","\n","        # --- Extra safety: force same (h, w) if shapes differ slightly ---\n","        if crop_np.shape[:2] != mask_local.shape[:2]:\n","            h = min(crop_np.shape[0], mask_local.shape[0])\n","            w = min(crop_np.shape[1], mask_local.shape[1])\n","            crop_np = crop_np[:h, :w, :]\n","            mask_local = mask_local[:h, :w]\n","\n","        # --- Apply instance mask (zero out background) if requested ---\n","        if self.use_mask and mask_local.sum() > 0:\n","            mask_local_3 = np.repeat(mask_local[:, :, None], 3, axis=2)  # (Hc, Wc, 3)\n","            crop_np = np.where(mask_local_3, crop_np, 0)\n","\n","        crop_img = Image.fromarray(crop_np)\n","        tensor = self.transform(crop_img)\n","        label = self.class2idx[s[\"class_name\"]]\n","\n","        if self.return_meta:\n","            meta = {\n","                \"img_path\": s[\"img_path\"],\n","                \"ann_path\": s[\"ann_path\"],\n","                \"class_name\": s[\"class_name\"],\n","                \"class_id\": s[\"class_id\"],\n","                \"bbox\": s[\"bbox\"],\n","            }\n","            return tensor, label, meta\n","        else:\n","            return tensor, label\n","\n","\n","# ↓↓↓ Smaller batch sizes to avoid OOM when unfreezing ↓↓↓\n","BATCH_TRAIN = 24\n","BATCH_VAL   = 32   # val has no grads, can be a bit larger\n","\n","train_ds = SegmentDataset(train_segments, class2idx, train_transform, use_mask=True, return_meta=False)\n","val_ds   = SegmentDataset(val_segments,   class2idx, val_transform,   use_mask=True, return_meta=False)\n","\n","dl_tr = DataLoader(\n","    train_ds,\n","    batch_size=BATCH_TRAIN,\n","    shuffle=True,\n","    num_workers=4,\n","    pin_memory=True,\n",")\n","dl_va = DataLoader(\n","    val_ds,\n","    batch_size=BATCH_VAL,\n","    shuffle=False,\n","    num_workers=4,\n","    pin_memory=True,\n",")\n","\n","print(\"Train batches:\", len(dl_tr), \"| Val batches:\", len(dl_va))\n","\n","# Optional quick check\n","batch = next(iter(dl_tr))\n","imgs, labels = batch\n","print(\"Sanity check batch shapes -> imgs:\", imgs.shape, \"| labels:\", labels.shape)\n","\n","gpu_clear()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ECu4OspSxijr","executionInfo":{"status":"ok","timestamp":1764258465282,"user_tz":360,"elapsed":2500,"user":{"displayName":"Shaik Mathin","userId":"03614909128096919146"}},"outputId":"fdc17e47-ecc8-4857-da03-435c6f3119db"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Num classes: 103\n","Example class2idx entries: [('French beans', 0), ('almond', 1), ('apple', 2), ('apricot', 3), ('asparagus', 4), ('avocado', 5), ('bamboo shoots', 6), ('banana', 7), ('bean sprouts', 8), ('biscuit', 9)]\n","Train batches: 366 | Val batches: 68\n","Sanity check batch shapes -> imgs: torch.Size([24, 3, 288, 288]) | labels: torch.Size([24])\n"]}]},{"cell_type":"code","source":["# ==================================================\n","# Cell 8: Build class_weights for balanced CrossEntropy\n","#   >>> RUN THIS BEFORE THE CONVNEXT TRAINING CELL <<<\n","# ==================================================\n","\n","from collections import Counter\n","\n","train_counts = Counter(s[\"class_name\"] for s in train_segments)\n","\n","class_freq = np.zeros(NUM_CLASSES, dtype=np.float32)\n","for name, cnt in train_counts.items():\n","    idx = class2idx[name]\n","    class_freq[idx] = cnt\n","\n","class_freq[class_freq == 0] = 1.0\n","class_weights_np = 1.0 / np.sqrt(class_freq)\n","class_weights = torch.tensor(class_weights_np, dtype=torch.float32, device=DEVICE)\n","\n","print(\"Built class_weights with shape:\", class_weights.shape)\n","print(\"First 10 class_freq:\", class_freq[:10])\n","print(\"First 10 class_weights:\", class_weights[:10])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hK393TI4xkrE","executionInfo":{"status":"ok","timestamp":1764258465288,"user_tz":360,"elapsed":4,"user":{"displayName":"Shaik Mathin","userId":"03614909128096919146"}},"outputId":"c56bf133-ba79-4991-e112-b15e977f0d2a"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Built class_weights with shape: torch.Size([103])\n","First 10 class_freq: [131.  31.  50.   8.  88.  24.   4.  52.  12. 105.]\n","First 10 class_weights: tensor([0.0874, 0.1796, 0.1414, 0.3536, 0.1066, 0.2041, 0.5000, 0.1387, 0.2887,\n","        0.0976], device='cuda:0')\n"]}]},{"cell_type":"code","source":["# ==================================================\n","# Cell 9: Load ConvNeXt Large checkpoint or train if missing\n","# ==================================================\n","\n","import torch.nn as nn\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","\n","CKPT_PATH = \"/content/drive/MyDrive/Cal_Estimation_Project/checkpoints/convnext_large_foodseg103_masked_ema.pth\"\n","SKIP_TRAIN_IF_CKPT = True\n","\n","print(\"Looking for checkpoint at:\", CKPT_PATH)\n","print(\"Exists:\", os.path.exists(CKPT_PATH))\n","\n","\n","def build_model():\n","    model = timm.create_model(\"convnext_large\", pretrained=True, num_classes=NUM_CLASSES)\n","    return model\n","\n","\n","def evaluate_model(model, loader):\n","    model.eval()\n","    criterion = nn.CrossEntropyLoss().to(DEVICE)\n","    correct = 0\n","    total = 0\n","    total_loss = 0.0\n","\n","    with torch.no_grad():\n","        for xb, yb in loader:\n","            xb = xb.to(DEVICE, non_blocking=True)\n","            yb = yb.to(DEVICE, non_blocking=True)\n","            with autocast(enabled=(DEVICE.type == \"cuda\")):\n","                logits = model(xb)\n","                loss = criterion(logits, yb)\n","            preds = logits.argmax(dim=1)\n","            correct += int((preds == yb).sum().item())\n","            total += int(yb.size(0))\n","            total_loss += float(loss.item()) * xb.size(0)\n","\n","    acc = correct / max(total, 1)\n","    avg_loss = total_loss / max(total, 1)\n","    return acc, avg_loss\n","\n","\n","if SKIP_TRAIN_IF_CKPT and os.path.exists(CKPT_PATH):\n","    print(\"Checkpoint found. Loading EMA model. Skipping training.\")\n","    ema_eval_model = build_model().to(DEVICE)\n","    ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n","\n","    if \"ema_state_dict\" in ckpt:\n","        state_dict = ckpt[\"ema_state_dict\"]\n","    elif \"model_state_dict\" in ckpt:\n","        state_dict = ckpt[\"model_state_dict\"]\n","    else:\n","        raise KeyError(\"Checkpoint does not contain model weights.\")\n","\n","    ema_eval_model.load_state_dict(state_dict)\n","    ema_eval_model.eval()\n","\n","    val_acc, val_loss = evaluate_model(ema_eval_model, dl_va)\n","    print(\"Loaded EMA checkpoint.\")\n","    print(\"ValAcc:\", val_acc, \"| ValLoss:\", val_loss)\n","else:\n","    print(\"No checkpoint found. Training will start.\")\n","\n","    EPOCHS = 20\n","    LR = 3e-4\n","    WD = 1e-2\n","    EARLY_PATIENCE = 6\n","    WARMUP_EPOCHS = 2\n","\n","    model = build_model().to(DEVICE)\n","    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WD)\n","    scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n","    criterion = nn.CrossEntropyLoss(weight=class_weights).to(DEVICE)\n","    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS - WARMUP_EPOCHS)\n","\n","    ema_model = build_model().to(DEVICE)\n","    ema_model.load_state_dict(model.state_dict())\n","    ema_decay = 0.999\n","\n","    def update_ema(ema_model, model, alpha):\n","        with torch.no_grad():\n","            for ep, p in zip(ema_model.parameters(), model.parameters()):\n","                ep.data.mul_(alpha).add_(p.data, alpha=1 - alpha)\n","\n","    best_val = 0.0\n","    best_epoch = -1\n","    no_improve = 0\n","\n","    def run_epoch(train=True):\n","        loader = dl_tr if train else dl_va\n","        model.train() if train else model.eval()\n","\n","        correct = 0\n","        total = 0\n","        running_loss = 0.0\n","\n","        for xb, yb in loader:\n","            xb = xb.to(DEVICE, non_blocking=True)\n","            yb = yb.to(DEVICE, non_blocking=True)\n","\n","            if train:\n","                optimizer.zero_grad(set_to_none=True)\n","\n","            with autocast(enabled=(DEVICE.type == \"cuda\")):\n","                logits = model(xb)\n","                loss = criterion(logits, yb)\n","\n","            if train:\n","                scaler.scale(loss).backward()\n","                scaler.step(optimizer)\n","                scaler.update()\n","                update_ema(ema_model, model, ema_decay)\n","\n","            preds = logits.argmax(dim=1)\n","            correct += int((preds == yb).sum().item())\n","            total += int(yb.size(0))\n","            running_loss += float(loss.item()) * xb.size(0)\n","\n","        acc = correct / max(total, 1)\n","        avg_loss = running_loss / max(total, 1)\n","        return acc, avg_loss\n","\n","    print(\"Starting ConvNeXt Large training...\")\n","    for epoch in range(1, EPOCHS + 1):\n","        train_acc, train_loss = run_epoch(train=True)\n","        val_acc, val_loss = run_epoch(train=False)\n","\n","        if epoch > WARMUP_EPOCHS:\n","            scheduler.step()\n","\n","        print(f\"Epoch {epoch}/{EPOCHS} | TrainAcc={train_acc:.3f} | ValAcc={val_acc:.3f}\")\n","\n","        if val_acc > best_val + 1e-4:\n","            best_val = val_acc\n","            best_epoch = epoch\n","            no_improve = 0\n","\n","            torch.save(\n","                {\n","                    \"ema_state_dict\": ema_model.state_dict(),\n","                    \"model_state_dict\": model.state_dict(),\n","                    \"best_val_acc\": best_val,\n","                    \"epoch\": epoch,\n","                },\n","                CKPT_PATH,\n","            )\n","            print(\"New best model saved.\")\n","        else:\n","            no_improve += 1\n","            print(\"No improvement.\")\n","            if no_improve >= EARLY_PATIENCE:\n","                print(\"Early stopping.\")\n","                break\n","\n","    ema_eval_model = ema_model\n","    ema_eval_model.eval()\n","    val_acc, val_loss = evaluate_model(ema_eval_model, dl_va)\n","    print(\"Training complete. Best ValAcc:\", best_val, \"at epoch\", best_epoch)\n","    print(\"Final EMA ValAcc:\", val_acc, \"ValLoss:\", val_loss)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r17NWWdwxnOx","executionInfo":{"status":"ok","timestamp":1764258506523,"user_tz":360,"elapsed":41234,"user":{"displayName":"Shaik Mathin","userId":"03614909128096919146"}},"outputId":"daea2113-9044-40dc-a569-719f642546fc"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking for checkpoint at: /content/drive/MyDrive/Cal_Estimation_Project/checkpoints/convnext_large_foodseg103_masked_ema.pth\n","Exists: True\n","Checkpoint found. Loading EMA model. Skipping training.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2112478679.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast(enabled=(DEVICE.type == \"cuda\")):\n"]},{"output_type":"stream","name":"stdout","text":["Loaded EMA checkpoint.\n","ValAcc: 0.9081632653061225 | ValLoss: 0.734186805909109\n"]}]},{"cell_type":"code","source":["# ==================================================\n","# Cell 10: Visualize 5 classified images + export PNGs & JSON\n","# ==================================================\n","\n","import json\n","\n","EXPORT_ROOT = PROJECT_ROOT / \"Outputs\" / \"classification_visuals\"\n","EXPORT_IMG_DIR = EXPORT_ROOT / \"images\"\n","EXPORT_JSON_DIR = EXPORT_ROOT / \"json\"\n","\n","EXPORT_IMG_DIR.mkdir(parents=True, exist_ok=True)\n","EXPORT_JSON_DIR.mkdir(parents=True, exist_ok=True)\n","\n","print(\"Image exports ->\", EXPORT_IMG_DIR)\n","print(\"JSON exports  ->\", EXPORT_JSON_DIR)\n","\n","\n","def ensure_ema_model():\n","    global ema_eval_model\n","    try:\n","        ema_eval_model\n","    except NameError:\n","        print(\"ema_eval_model not in memory, loading best EMA checkpoint...\")\n","        ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n","        ema_eval_model_local = build_model().to(DEVICE)\n","        ema_eval_model_local.load_state_dict(ckpt[\"ema_state_dict\"])\n","        ema_eval_model = ema_eval_model_local\n","    ema_eval_model.eval()\n","    ema_eval_model.to(DEVICE)\n","\n","\n","val_img_to_indices = defaultdict(list)\n","for i, s in enumerate(val_segments):\n","    val_img_to_indices[s[\"img_path\"]].append(i)\n","\n","val_meta_ds = SegmentDataset(\n","    val_segments, class2idx, val_transform, use_mask=True, return_meta=True\n",")\n","\n","\n","def make_segmentation_overlay(orig_np, mask_full):\n","    if mask_full.ndim == 3:\n","        mask_full = mask_full[:, :, 0]\n","\n","    overlay = orig_np.copy().astype(np.float32)\n","    unique_ids = [cid for cid in np.unique(mask_full) if cid != 0]\n","    rng = np.random.RandomState(0)\n","    color_map = {cid: rng.randint(0, 255, size=3) for cid in unique_ids}\n","    alpha = 0.5\n","    for cid in unique_ids:\n","        color = color_map[cid]\n","        mask = (mask_full == cid)\n","        overlay[mask] = alpha * color.astype(np.float32) + (1 - alpha) * overlay[mask]\n","    return np.clip(overlay, 0, 255).astype(np.uint8)\n","\n","\n","def visualize_full_image_predictions(num_images=5, max_foods_per_image=6):\n","    ensure_ema_model()\n","\n","    if len(val_img_to_indices) == 0:\n","        print(\"No validation segments available.\")\n","        return\n","\n","    img_paths = list(val_img_to_indices.keys())\n","    num_images = min(num_images, len(img_paths))\n","    chosen_paths = random.sample(img_paths, k=num_images)\n","\n","    for img_idx, img_path in enumerate(chosen_paths):\n","        seg_indices = val_img_to_indices[img_path][:max_foods_per_image]\n","\n","        orig = Image.open(img_path).convert(\"RGB\")\n","        orig_np = np.array(orig)\n","\n","        ann_path = val_segments[seg_indices[0]][\"ann_path\"]\n","        mask_full = np.array(Image.open(ann_path))\n","        if mask_full.ndim == 3:\n","            mask_full = mask_full[:, :, 0]\n","\n","        seg_overlay = make_segmentation_overlay(orig_np, mask_full)\n","        n_foods = len(seg_indices)\n","        if n_foods == 0:\n","            print(f\"No segments for image: {img_path}\")\n","            continue\n","\n","        n_rows = 1 + n_foods\n","        n_cols = 2\n","        plt.figure(figsize=(6 * n_cols, 3 * n_rows))\n","\n","        ax0 = plt.subplot(n_rows, n_cols, 1)\n","        ax0.imshow(orig_np)\n","        ax0.set_title(\"Original Image\")\n","        ax0.axis(\"off\")\n","\n","        ax1 = plt.subplot(n_rows, n_cols, 2)\n","        ax1.imshow(seg_overlay)\n","        ax1.set_title(\"Segmented Image (all classes)\")\n","        ax1.axis(\"off\")\n","\n","        for row_i, seg_idx in enumerate(seg_indices, start=1):\n","            img_tensor, label, meta = val_meta_ds[seg_idx]\n","            true_name = meta[\"class_name\"]\n","            cid = meta[\"class_id\"]\n","            x0, y0, x1, y1 = meta[\"bbox\"]\n","\n","            img_np_crop = img_tensor.numpy().transpose(1, 2, 0)\n","            mean = np.array([0.485, 0.456, 0.406])\n","            std  = np.array([0.229, 0.224, 0.225])\n","            img_np_crop = std * img_np_crop + mean\n","            img_np_crop = np.clip(img_np_crop, 0.0, 1.0)\n","\n","            mask_full_local = mask_full.copy()\n","            mask_instance = (mask_full_local == cid)\n","            mask_instance_box = np.zeros_like(mask_instance)\n","            mask_instance_box[y0:y1+1, x0:x1+1] = mask_instance[y0:y1+1, x0:x1+1]\n","\n","            inst_overlay = orig_np.copy().astype(np.float32)\n","            alpha_inst = 0.6\n","            color_inst = np.array([255, 0, 0])\n","            inst_overlay[mask_instance_box] = (\n","                alpha_inst * color_inst + (1 - alpha_inst) * inst_overlay[mask_instance_box]\n","            )\n","            inst_overlay = np.clip(inst_overlay, 0, 255).astype(np.uint8)\n","\n","            with torch.no_grad():\n","                inp = img_tensor.unsqueeze(0).to(DEVICE)\n","                with autocast(enabled=(DEVICE.type == \"cuda\")):\n","                    logits = ema_eval_model(inp)\n","                probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n","                pred_idx = int(probs.argmax())\n","                pred_name = idx2class[pred_idx]\n","                conf = float(probs[pred_idx])\n","\n","            ax_left = plt.subplot(n_rows, n_cols, row_i * n_cols + 1)\n","            ax_left.imshow(inst_overlay)\n","            ax_left.set_title(f\"Food {row_i} mask\\nGT: {true_name}\", fontsize=9)\n","            ax_left.axis(\"off\")\n","\n","            ax_right = plt.subplot(n_rows, n_cols, row_i * n_cols + 2)\n","            ax_right.imshow(img_np_crop)\n","            ax_right.set_title(\n","                f\"P: {pred_name}\\nT: {true_name}\\nConf: {conf:.2f}\", fontsize=9\n","            )\n","            ax_right.axis(\"off\")\n","\n","        plt.tight_layout()\n","        plt.show()\n","        print(f\"[VIS] Visualized image {img_idx+1}/{num_images}: {img_path}\")\n","\n","\n","def export_visuals_and_json_all(\n","    max_foods_per_image=8,\n","    export_mode=\"all_json_png_some\",\n","    max_png_images=50,\n","):\n","    ensure_ema_model()\n","\n","    img_paths = list(val_img_to_indices.keys())\n","    total_images = len(img_paths)\n","    print(f\"Found {total_images} validation images for export.\")\n","\n","    png_limit = total_images if export_mode == \"all_json_png_all\" else max_png_images\n","    print(f\"PNG export mode: {export_mode} (up to {png_limit} PNGs)\")\n","\n","    for img_idx, img_path in enumerate(img_paths):\n","        seg_indices = val_img_to_indices[img_path][:max_foods_per_image]\n","        orig = Image.open(img_path).convert(\"RGB\")\n","        orig_np = np.array(orig)\n","\n","        ann_path = val_segments[seg_indices[0]][\"ann_path\"]\n","        mask_full = np.array(Image.open(ann_path))\n","        if mask_full.ndim == 3:\n","            mask_full = mask_full[:, :, 0]\n","\n","        seg_overlay = make_segmentation_overlay(orig_np, mask_full)\n","        n_foods = len(seg_indices)\n","        if n_foods == 0:\n","            print(f\"[WARN] No segments for {img_path}\")\n","            continue\n","\n","        img_json = {\n","            \"image_path\": img_path,\n","            \"annotation_path\": ann_path,\n","            \"segments\": []\n","        }\n","\n","        do_png = img_idx < png_limit\n","        fig = None\n","        if do_png:\n","            n_rows = 1 + n_foods\n","            n_cols = 2\n","            fig = plt.figure(figsize=(6 * n_cols, 3 * n_rows))\n","\n","            ax0 = plt.subplot(n_rows, n_cols, 1)\n","            ax0.imshow(orig_np)\n","            ax0.set_title(\"Original Image\")\n","            ax0.axis(\"off\")\n","\n","            ax1 = plt.subplot(n_rows, n_cols, 2)\n","            ax1.imshow(seg_overlay)\n","            ax1.set_title(\"Segmented Image (all classes)\")\n","            ax1.axis(\"off\")\n","\n","        for row_i, seg_idx in enumerate(seg_indices, start=1):\n","            img_tensor, label, meta = val_meta_ds[seg_idx]\n","            true_name = meta[\"class_name\"]\n","            cid = meta[\"class_id\"]\n","            x0, y0, x1, y1 = meta[\"bbox\"]\n","\n","            img_np_crop = img_tensor.numpy().transpose(1, 2, 0)\n","            mean = np.array([0.485, 0.456, 0.406])\n","            std  = np.array([0.229, 0.224, 0.225])\n","            img_np_crop = std * img_np_crop + mean\n","            img_np_crop = np.clip(img_np_crop, 0.0, 1.0)\n","\n","            mask_instance = (mask_full == cid)\n","            mask_instance_box = np.zeros_like(mask_instance)\n","            mask_instance_box[y0:y1+1, x0:x1+1] = mask_instance[y0:y1+1, x0:x1+1]\n","\n","            inst_overlay = orig_np.copy().astype(np.float32)\n","            alpha_inst = 0.6\n","            color_inst = np.array([255, 0, 0])\n","            inst_overlay[mask_instance_box] = (\n","                alpha_inst * color_inst + (1 - alpha_inst) * inst_overlay[mask_instance_box]\n","            )\n","            inst_overlay = np.clip(inst_overlay, 0, 255).astype(np.uint8)\n","\n","            with torch.no_grad():\n","                inp = img_tensor.unsqueeze(0).to(DEVICE)\n","                with autocast(enabled=(DEVICE.type == \"cuda\")):\n","                    logits = ema_eval_model(inp)\n","                probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n","                pred_idx = int(probs.argmax())\n","                pred_name = idx2class[pred_idx]\n","                conf = float(probs[pred_idx])\n","\n","            if do_png and fig is not None:\n","                ax_left = plt.subplot(n_rows, n_cols, row_i * n_cols + 1)\n","                ax_left.imshow(inst_overlay)\n","                ax_left.set_title(f\"Food {row_i} mask\\nGT: {true_name}\", fontsize=9)\n","                ax_left.axis(\"off\")\n","\n","                ax_right = plt.subplot(n_rows, n_cols, row_i * n_cols + 2)\n","                ax_right.imshow(img_np_crop)\n","                ax_right.set_title(\n","                    f\"P: {pred_name}\\nT: {true_name}\\nConf: {conf:.2f}\",\n","                    fontsize=9,\n","                )\n","                ax_right.axis(\"off\")\n","\n","            mask_area = int(mask_instance_box.sum())\n","            img_json[\"segments\"].append(\n","                {\n","                    \"segment_index\": int(seg_idx),\n","                    \"class_name_pred\": pred_name,\n","                    \"class_name_true\": true_name,\n","                    \"class_id\": int(cid),\n","                    \"confidence\": conf,\n","                    \"bbox\": [int(x0), int(y0), int(x1), int(y1)],\n","                    \"mask_area\": mask_area,\n","                    \"volume\": None,\n","                    \"mass\": None,\n","                    \"calories\": None,\n","                }\n","            )\n","\n","        img_stem = os.path.splitext(os.path.basename(img_path))[0]\n","\n","        if do_png and fig is not None:\n","            plt.tight_layout()\n","            fig_path = EXPORT_IMG_DIR / f\"{img_stem}_classification.png\"\n","            plt.savefig(fig_path, bbox_inches=\"tight\", dpi=150)\n","            plt.close(fig)\n","            print(f\"[{img_idx+1}/{total_images}] PNG  -> {fig_path.name}\")\n","        elif fig is not None:\n","            plt.close(fig)\n","\n","        json_path = EXPORT_JSON_DIR / f\"{img_stem}_classification.json\"\n","        with open(json_path, \"w\") as f:\n","            json.dump(img_json, f, indent=2)\n","        print(f\"[{img_idx+1}/{total_images}] JSON -> {json_path.name}\")\n","\n","    print(\"\\nExport (all images) completed.\")\n","\n","\n","# ---- RUN VISUALIZATION + EXPORT ----\n","visualize_full_image_predictions(num_images=5, max_foods_per_image=6)\n","\n","export_visuals_and_json_all(\n","    max_foods_per_image=8,\n","    export_mode=\"all_json_png_some\",\n","    max_png_images=50,\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Dggamiejn6oefeOdOgsFDpPNLhF5l5u0"},"id":"TCP8xaYtxpUS","executionInfo":{"status":"ok","timestamp":1764258735856,"user_tz":360,"elapsed":229331,"user":{"displayName":"Shaik Mathin","userId":"03614909128096919146"}},"outputId":"cd993ba0-925a-4f27-9b19-c4d33f75195b"},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["# ==================================================\n","# Cell 11: Save cache for fast re-run (NEW)\n","# ==================================================\n","\n","cache_data = {\n","    \"checkpoint_path\": str(CKPT_PATH),\n","    \"num_segments_total\": len(segments),\n","    \"num_train_segments\": len(train_segments),\n","    \"num_val_segments\": len(val_segments),\n","    \"num_classes\": NUM_CLASSES,\n","    \"class2idx\": class2idx,\n","    \"idx2class\": idx2class,\n","    \"export_root\": str(EXPORT_ROOT),\n","}\n","\n","save_cache(cache_data, \"05_cache\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r3nSxL6n_KbG","executionInfo":{"status":"ok","timestamp":1764258834655,"user_tz":360,"elapsed":9,"user":{"displayName":"Shaik Mathin","userId":"03614909128096919146"}},"outputId":"951195cb-bd11-4a75-f7ec-25f91a595678"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["✔ Saved cache → /content/drive/MyDrive/Cal_Estimation_Project/cache/05_cache.pkl\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"2f-Xe5eZBWfP"},"execution_count":null,"outputs":[]}]}